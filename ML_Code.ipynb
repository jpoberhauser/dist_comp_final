{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!aws s3 cp s3://msan694-group/final_nba.csv final_nba.csv\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label,SHOT_NUMBER,DRIBBLES,TOUCH_TIME,SHOT_DIST,PTS_TYPE,CLOSE_DEF_DIST,home,time\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'label,SHOT_NUMBER,DRIBBLES,TOUCH_TIME,SHOT_DIST,PTS_TYPE,CLOSE_DEF_DIST,home,time',\n",
       " u'1,1,2,1.9,7.7,2,1.3,0,10.85',\n",
       " u'0,2,0,0.8,28.2,3,6.1,0,11.76666667',\n",
       " u'0,3,3,2.7,10.1,2,0.9,0,12',\n",
       " u'0,4,2,1.9,17.2,2,3.4,0,12.21666667']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_vec = sc.textFile(\"final_nba.csv\")\n",
    "header = rdd_vec.first() #extract header\n",
    "print header\n",
    "rdd_vec.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128069"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = rdd_vec.filter(lambda row: row not in header) #filter out the header!\n",
    "new_rdd = data.map(lambda line: line.split(',')) # split the wide vector by \",\"\n",
    "new_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label,SHOT_NUMBER,DRIBBLES,TOUCH_TIME,SHOT_DIST,PTS_TYPE,CLOSE_DEF_DIST,home,time\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|[1.0,2.0,1.9,7.7,...|\n",
      "|  0.0|[2.0,0.0,0.8,28.2...|\n",
      "|  0.0|[3.0,3.0,2.7,10.1...|\n",
      "|  0.0|[4.0,2.0,1.9,17.2...|\n",
      "|  0.0|[5.0,2.0,2.7,3.7,...|\n",
      "|  0.0|[6.0,2.0,4.4,18.4...|\n",
      "|  0.0|[7.0,11.0,9.0,20....|\n",
      "|  1.0|[8.0,3.0,2.5,3.5,...|\n",
      "|  0.0|[9.0,0.0,0.8,24.6...|\n",
      "|  0.0|[1.0,0.0,1.1,22.4...|\n",
      "|  0.0|[2.0,8.0,7.5,24.5...|\n",
      "|  1.0|[3.0,14.0,11.9,14...|\n",
      "|  1.0|[4.0,2.0,2.9,5.9,...|\n",
      "|  0.0|[1.0,0.0,0.8,26.4...|\n",
      "|  0.0|[1.0,0.0,0.5,22.8...|\n",
      "|  1.0|[2.0,3.0,2.7,24.7...|\n",
      "|  0.0|[3.0,6.0,5.1,25.0...|\n",
      "|  0.0|[4.0,1.0,0.9,25.6...|\n",
      "|  1.0|[5.0,0.0,1.2,24.2...|\n",
      "|  0.0|[1.0,2.0,2.2,25.4...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#turn to Vectors.dense (with label out front)\n",
    "split_rdd = new_rdd.map(lambda line: (float(line[0]), Vectors.dense([float(c) for c in line[1:len(line)]]))) \n",
    "\n",
    "# Create the DataFrame from the collected RDD\n",
    "full_df = sqlContext.createDataFrame(split_rdd.collect(), [\"label\", \"features\"])\n",
    "full_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.1 ms, sys: 880 Âµs, total: 2.98 ms\n",
      "Wall time: 155 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(trainingData, testData) = full_df.randomSplit([0.7, 0.3])\n",
    "trainingData = trainingData.cache()\n",
    "testData = testData.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(maxIter=2, maxDepth=2, labelCol=\"label\")\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+\n",
      "|label|            features|prediction|\n",
      "+-----+--------------------+----------+\n",
      "|  0.0|[1.0,0.0,0.0,1.5,...|       1.0|\n",
      "|  0.0|[1.0,0.0,0.0,2.2,...|       1.0|\n",
      "|  0.0|[1.0,0.0,0.0,24.5...|       0.0|\n",
      "|  0.0|[1.0,0.0,0.1,1.2,...|       1.0|\n",
      "|  0.0|[1.0,0.0,0.2,4.1,...|       1.0|\n",
      "|  0.0|[1.0,0.0,0.4,4.1,...|       1.0|\n",
      "|  0.0|[1.0,0.0,0.6,3.7,...|       1.0|\n",
      "|  0.0|[1.0,0.0,0.6,6.6,...|       0.0|\n",
      "|  0.0|[1.0,0.0,0.6,7.6,...|       0.0|\n",
      "|  0.0|[1.0,0.0,0.6,15.3...|       0.0|\n",
      "|  0.0|[1.0,0.0,0.6,15.6...|       0.0|\n",
      "|  0.0|[1.0,0.0,0.6,17.3...|       0.0|\n",
      "|  0.0|[1.0,0.0,0.6,24.3...|       0.0|\n",
      "|  0.0|[1.0,0.0,0.6,25.1...|       0.0|\n",
      "|  0.0|[1.0,0.0,0.7,1.2,...|       1.0|\n",
      "|  0.0|[1.0,0.0,0.7,2.9,...|       1.0|\n",
      "|  0.0|[1.0,0.0,0.7,5.0,...|       1.0|\n",
      "|  0.0|[1.0,0.0,0.7,6.3,...|       0.0|\n",
      "|  0.0|[1.0,0.0,0.7,10.1...|       0.0|\n",
      "|  0.0|[1.0,0.0,0.7,13.7...|       0.0|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "predictions.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 59.2201\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# n-fold validation and the results.\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "cv = CrossValidator().setEstimator(gbt).setEvaluator(evaluator).setNumFolds(5)\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxDepth, [3,6]).addGrid(gbt.maxIter, [4,20]).build()\n",
    "cv.setEstimatorParamMaps(paramGrid)\n",
    "cvmodel = cv.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.616441908713693"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(cvmodel.bestModel.transform(testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GBTClassificationModel (uid=GBTClassifier_4b05842593e4a7f2e2ac) with 4 trees"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvmodel.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train the model.\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(regParam=0.01, maxIter=1000, fitIntercept=True)\n",
    "lrmodel = lr.fit(trainingData)\n",
    "lrmodel = lr.setParams(regParam=0.01, maxIter=500, fitIntercept=True).fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|[1.0,0.0,0.0,1.5,...|[-0.3641878432672...|[0.40994618653634...|       1.0|\n",
      "|  0.0|[1.0,0.0,0.0,2.2,...|[-0.3613138932369...|[0.41064154668971...|       1.0|\n",
      "|  0.0|[1.0,0.0,0.0,24.5...|[0.44815089963004...|[0.61019950393247...|       0.0|\n",
      "|  0.0|[1.0,0.0,0.1,1.2,...|[-0.4337087769319...|[0.39324105851018...|       1.0|\n",
      "|  0.0|[1.0,0.0,0.2,4.1,...|[-0.4323223519640...|[0.39357191193067...|       1.0|\n",
      "|  0.0|[1.0,0.0,0.4,4.1,...|[-0.1179658659393...|[0.47054268612612...|       1.0|\n",
      "|  0.0|[1.0,0.0,0.6,3.7,...|[-0.4332154709255...|[0.39335876876738...|       1.0|\n",
      "|  0.0|[1.0,0.0,0.6,6.6,...|[0.06772491382556...|[0.51692475993533...|       0.0|\n",
      "|  0.0|[1.0,0.0,0.6,7.6,...|[-0.2229847253367...|[0.44448366108112...|       1.0|\n",
      "|  0.0|[1.0,0.0,0.6,15.3...|[0.31844815118794...|[0.57894600821576...|       0.0|\n",
      "|  0.0|[1.0,0.0,0.6,15.6...|[0.09802911480132...|[0.52448767189366...|       0.0|\n",
      "|  0.0|[1.0,0.0,0.6,17.3...|[0.50307654208982...|[0.62318205715571...|       0.0|\n",
      "|  0.0|[1.0,0.0,0.6,24.3...|[0.81363086132000...|[0.69288267868379...|       0.0|\n",
      "|  0.0|[1.0,0.0,0.6,25.1...|[0.86500762300319...|[0.70370582742840...|       0.0|\n",
      "|  0.0|[1.0,0.0,0.7,1.2,...|[-0.3642353658072...|[0.40993469134358...|       1.0|\n",
      "|  0.0|[1.0,0.0,0.7,2.9,...|[-0.0683567564729...|[0.48291746208505...|       1.0|\n",
      "|  0.0|[1.0,0.0,0.7,5.0,...|[-0.2353130340844...|[0.44144170104362...|       1.0|\n",
      "|  0.0|[1.0,0.0,0.7,6.3,...|[-0.1732533542121...|[0.45679468093285...|       1.0|\n",
      "|  0.0|[1.0,0.0,0.7,10.1...|[0.13533519540957...|[0.53378225269049...|       0.0|\n",
      "|  0.0|[1.0,0.0,0.7,13.7...|[0.20395111676450...|[0.55081177039692...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluate models using test dataset.\n",
    "validpredicts = lrmodel.transform(testData)\n",
    "validpredicts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC:0.630023701057\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model. default metric : Area Under ROC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "bceval = BinaryClassificationEvaluator()\n",
    "print (bceval.getMetricName() +\":\" + str(bceval.evaluate(validpredicts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n-fold validation and the results.\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "cv = CrossValidator().setEstimator(lr).setEvaluator(bceval).setNumFolds(5)\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.maxIter, [1000]).addGrid(lr.regParam, [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5]).build()\n",
    "cv.setEstimatorParamMaps(paramGrid)\n",
    "cvmodel = cv.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6300237010572356"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BinaryClassificationEvaluator().evaluate(cvmodel.bestModel.transform(testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GBTClassificationModel (uid=GBTClassifier_4b05842593e4a7f2e2ac) with 4 trees"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvmodel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
